---
title: "Prototype Analysis for Inferring Treatment Impact - ACCEL dataset"
author: "Laurens Geffert"
date: "2023-01-10"
output: rmarkdown::github_document
---

<!-- impact_of_treatment.md is generated from impact_of_treatment.Rmd Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = '##',
  fig.retina = 2,
  fig.path = '../output/impact_of_treatment_files/impact_of_treatment_accel')
```

```{r setup, echo = FALSE, error = TRUE}
library(sf)
library(fs)
library(raster)
library(tidyverse)
library(tidymodels)
library(magrittr)
library(purrr)
library(zeallot)
library(mice)
```


We're using the ACCEL dataset because it includes raw values. The analysis should be replicated with RRK data at a later stage. Some of the data needs some pre-processing. Climate class is not at the same resolution and extent as the other layers. All layers are stacked and masked to the Sierra Nevada study area. Finally, we generate a random sample of cells to keep processing times manageable.

```{r defining_inputs, error = TRUE}
# Using this raster as the target resolution and extent
default_raster <- raster('../data/ACCEL RRK - all layers/airQuality/particulate/PotentialSmokeHighSeverity_2021_300m_base.tif')

resample_if_needed <- function(r, default_raster, silent = TRUE, ...) {
  if (!all(dim(r) == dim(default_raster))) {
    if (!compareCRS(r, default_raster)) {
      message(paste('reprojecting raster', names(r)))
      r <- projectRaster(from = r, to = default_raster)
    }
    message(paste('resampling raster', names(r)))
    r <- raster::resample(x = r, y = default_raster, 'ngb')
  } else {
    if (!silent) message(paste('no need to resample', names(r)))
  }
  return(r)
}

# getting file paths of all metrics
metrics <- tibble(
  filename = '../data/ACCEL RRK - all layers/' |>
    dir_ls(recurse = TRUE) |>
    str_subset('(\\.tif$)|(\\.img$)'))

# getting name hierarchy
metrics <- metrics |>
  mutate(
    subfolder = str_extract(
      string = filename,
      pattern = '(?<=\\.\\.\\/data\\/ACCEL RRK - all layers\\/).+(?=(\\.tif$)|(\\.img$))')) |>
  separate(
    col = subfolder,
    into = c('pillar', 'element', 'folder', 'metric'),
    sep = '/',
    fill = 'left') |>
  mutate(
    folder = coalesce(folder, metric),
    element = coalesce(element, folder),
    pillar = coalesce(pillar, element)) |>
  # filtering out 30m rasters
  filter(str_detect(metric, '_30m_', negate = TRUE)) |>
  # a few layers aren't named consistently
  filter(!metric %in% c(
    'Mortality_MMI_2017_2021_normalized_5climateClass30m',
    'CA_Black_Oak_Stand_Distribution_2016to2020',
    'ACCEL_habitatConnectivity_valuesInt',
    'DamagePotential_WUI_2022',
    'StructureExposureScore_WUI_2022',
    'Mortality_MMI_2017_2021_compressed'
    )) |>
  # some layers are spatially restriced and thus introduce NAs to the data
  # TODO: Should these be set to 0 where NA instead?
  filter(!metric %in% c(
    'DamagePotential_WUI_2022_300m_base',
    'StructureExposureScore_WUI_2022_300m_base',
    'Meadow_SensNDWI_2019_300m_base'
    )) |>
  # filtering out normalized rasters
  filter(str_detect(metric, '_normalized$', negate = TRUE))

# adding climate class
metrics <- tibble(
  filename = '../data/Sierra Nevada ACCEL/ClimateClasses/ClimateClasses.img',
  pillar = 'climClass',
  element = 'climClass',
  folder = 'climClass',
  metric = 'climClass') |>
  bind_rows(metrics)
```


```{r loading_data, error = TRUE}
# loading rasters
rasters <- metrics |>
  mutate(r = filename |>
    map(.f = raster) |>
    map2(.y = metric, .f = ~set_names(.x, .y))) |>
  mutate(r = map(r, resample_if_needed, default_raster = default_raster))

# convert raster values into dataframe
df <- rasters %>%
  pluck('r') %>%
  stack() %>%
  values() %>%
  as_tibble()

# drop rows that are all-NA
df <- df[rowSums(!is.na(df)) > 0, ]
```


```{r}
# creating test and validation split
set.seed(42)
data_split <- initial_split(df, prop = 3/4)
df_train <- training(data_split)
df_test  <- testing(data_split)
```




```{r}
# create a sample that's easier to work with in memory
sierra_metrics <- c('bio_div_sr', 'bio_com_fgr', 'bio_foc_cso', 'zzz_zzz_climclass')
df <- x[sample(nrow(x), size = 1e7, replace = FALSE), ] %>%
  as_tibble() %>%
  # drop all rows that only contain NA for TCSI metrics
  filter(if_any(-one_of(sierra_metrics), ~ !is.na(.))) %>%
  # introducing sampling to speed up the code during dev TODO: take this out
  sample_n(1e4)

head(df)
```


Here we plot a histogram of the values for each metric. We're using only the
interpreted values (range -1 to 1). You can see that some of the variables are
approximately normally distributed while others look almost like binomial
distributions.

```{r plot_metric_histogram, error = TRUE}
# plot a histogram for each metric
df %>%
  pivot_longer(everything()) %>%
  filter(complete.cases(.)) %>%
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name, scales = 'free')
```


We can investigate correlations between metrics with a pair plot collection of
scatterplots. We're also adding a trend line to make it easier to spot
relationships between the metrics where many observations fall into the same
value range.

```{r pair_plot, error = TRUE}
df %>%
  # drop variables to save space on pair plot
  select(starts_with(c('for', 'fir'))) %>%
  select(-fir_fun_tslf) %>%
  # drop any rows with NAs
  filter(complete.cases(.)) %>%
  # self-join data to get pairs of observations
  mutate(row_id = row_number()) %>%
  pivot_longer(cols = -row_id) %>%
  full_join(., ., by = 'row_id') %>%
  # create pair plot
  ggplot( aes(x = value.x, y = value.y)) + 
  #geom_density2d_filled() +
  #scale_fill_viridis_d() +
  geom_point(alpha = .02) +
  geom_smooth(method = 'gam', alpha = .5, color = 'coral') +
  facet_grid(name.x ~ name.y, scales = 'free', switch = 'y') +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank())
```


To compare good and bad conditions more directly, we can restrict our analysis
to the top and bottom 10% of cell values. This allows us to see if the
distribution of other metrics changes substantially between e.g. good tree
density and bad tree density cells.

```{r find_good_bad_cells_td, error = TRUE}


# Find areas with top 10% tree density and bottom 10% tree density,
# then look at histograms for other metrics
td_good <- df$for_str_td %>% quantile(.9, na.rm = TRUE)
td_bad <- df$for_str_td %>% quantile(.1, na.rm = TRUE)

# Drop all cells with mediocre tree density
df_quality_td <- df %>%
  mutate(quality_td = case_when(
    for_str_td >= td_good ~ 'good',
    for_str_td <= td_bad ~ 'bad',
    TRUE ~ NA_character_)) %>%
  filter(!is.na(quality_td))

# plot distribution of other metrics for good and bad tree density cells
df_quality_td %>%
  pivot_longer(-quality_td) %>%
  ggplot(aes(x = value, group = quality_td, fill = quality_td)) +
  geom_density(alpha = .5) +
  facet_wrap(~name, scales = 'free')
```


The distribution plots above are helpful to understand the relationships between
different metrics, but it is infeasible to create and visually inspect them for
all metric pairs in the dataset. Therefore, we use two numeric values, the 
spearman rank correlation coefficient and the kolmogorov-smirnov D-statistic to
measure how interrelated two variables are.

```{r test_good_bad_difference, error = TRUE}

# filter data to only include top and bottom x percent for a given metric
filter_high_low <- function(df, var_name, quantile = .1) {
  
  message('\n\nsplitting by ', var_name)
  
  threshold_good <- df %>%
    pluck(var_name) %>%
    quantile(1 - quantile, na.rm = TRUE)
  threshold_bad <- df %>%
    pluck(var_name) %>%
    quantile(quantile, na.rm = TRUE)
  # over 90% of this metric are -1!
  if (var_name == 'bio_foc_cso') {
    threshold_good <- 1
  }
  df %>%
    mutate(quality = case_when(
      !!sym(var_name) >= threshold_good ~ 'good',
      !!sym(var_name) <= threshold_bad ~ 'bad',
      TRUE ~ 'neutral')) %>%
    filter(quality != 'neutral')
}

# run a kolmogorov-smirnov test to compare the distributions of one metric
# between the top and bottom value groups of another metric
ks_test <- function(df,
                    column_testing,
                    column_grouping = 'quality') {
  message('\ntesting for ', column_testing)
  out <- ks.test(
    x = df %>%
      filter(!!sym(column_grouping) == 'good') %>%
      pluck(column_testing),
    y = df %>%
      filter(!!sym(column_grouping) == 'bad') %>%
      pluck(column_testing))
  return(out)
}

# create all metric pairs
metrics <- df %>%
  select(-zzz_zzz_climclass) %>%
  colnames()
combinations <- crossing(
  metric.x = metrics,
  metric.y = metrics) %>% 
  filter(metric.x != metric.y)

# run ks-test for each metric pair
results <- combinations %>%
  mutate(ks_result = map2(
    .x = metric.x, .y = metric.y,
    .f = ~ df %>%
      filter_high_low(var_name = .x) %>%
      ks_test(column_testing = .y))) %>%
  mutate(
    spearman_cor = map2_dbl(
      .x = metric.x, .y = metric.y, 
      .f = ~ df %>%
        select(.x, .y) %>%
        filter(complete.cases(.)) %>%
        cor(method = 'spearman') %>%
        pluck(2))) %>%
  mutate(
    ks_D = map_dbl(ks_result, ~.x$statistic),
    ks_p = map_dbl(ks_result, ~.x$p.value)) %>%
  select(-ks_result)

results %>%
  arrange(ks_p, desc(ks_D))

write.csv(results,file = 'output/cor.csv')
```


We can try to use the relationships between the metrics to predict the likely
value of a metric AFTER treatment, given the values of the metrics BEFORE
treatment as well as the value of some selected metrics that we know will change
to a specific value as the DIRECT result of landscape treatment.

Here, we assume that *basal area*, *tree density*, and *large tree density*
may be changed as a direct result of landscape treatment. *Climate class* will
always remain unchanged. All other metrics may change as an indirect result of
treatment. We will try to estimate these direct impacts using a machine learning
approach. Specifically, we can use multiple linear model imputation from the 
`mice` package to achieve this.

Let's test this below:

```{r, error = TRUE}

df_imputation <- df %>%
  # convert climate class to factor so it can be used for imputation
  mutate(climclass = as.factor(zzz_zzz_climclass)) %>%
  select(-zzz_zzz_climclass) %>%
  # drop collinear metric
  select(-fir_fun_frid) %>%
  # drop incomplete cases for now
  filter(complete.cases(.))

# these variables will not change and therefore should not be masked / imputed
exclude <- c('climclass', 'for_str_td', 'for_str_ba', 'for_str_ltd')

# Drops random fields from metrics data frame. Returns masked data frame and an
# index list with the row + col indices of the values that were dropped
drop_random_values <- function(
    df,
    exclude = c('climclass', 'for_str_td', 'for_str_ba', 'for_str_ltd'),
    p = .2,
    seed = 42) {
  set.seed(seed)
  # drop columns that are excluded from sampling
  excluded <- df[, exclude]
  df <- select(df, -all_of(!!exclude))
  # sample of random rows with given probability
  sample_mask <- tibble(.rows = nrow(df))
  for (i in colnames(df)) {
    i_mask <- rbinom(nrow(df), 1, p)
    sample_mask <- sample_mask %>%
      {. == 1} %>%
      bind_cols(i_mask) %>%
      set_names(c(colnames(sample_mask), i))
    df[i_mask == 1, i] <- NA
  }
  df <- bind_cols(df, excluded)
  return(list(df, sample_mask))
}

# drop some random cells
c(df_masked, sample_mask) %<-% drop_random_values(df_imputation, p = .2)
# compare NAs in masked data to the original, and to the sample mask.
# Should all be TRUE! (unless original data contained NAs)
all((is.na(df_masked) & !is.na(df_imputation)) == sample_mask)

# start mice setup
ini <- mice(
  data = df_masked,
  # find metrics with minimum correlation for prediction
  pred = quickpred(df_masked, mincor = .1),
  maxit = 0)
# specify post-processing logic for mice
# since interpreted values should fall within [-1, 1] range.
# see https://www.gerkovink.com/miceVignettes/Passive_Post_processing/Passive_imputation_post_processing.html for details
post_processing <- ini$post
post_processing_logic <- 'imp[[j]][, i] <- squeeze(imp[[j]][, i], c(-1, 1))'
post_processing[1:length(post_processing)-1] <- post_processing_logic

# specify prediction relationship.
pred <- ini$predictorMatrix
# always use climate class
pred[, 'climclass'] <- 1

# fit imputation model
imputation <- mice(
  data = df_masked,
  method = 'norm.predict',
  post = post_processing,
  m = 1,
  maxit = 10,
  seed = 42,
  print = FALSE)
# run actual imputation
df_completed <- imputation %>%
  complete() %>%
  tibble()

df_comparison <- tibble()
for (i in colnames(df_completed)) {
  if (!i %in% exclude) {
    # find values that were removed
    gaps <- is.na(df_masked[[i]]) & !is.na(df_imputation[[i]])
    x_i <- tibble(
      variable = i,
      predicted = df_completed[[i]][gaps],
      actual = df_imputation[[i]][gaps])
    df_comparison <- bind_rows(df_comparison, x_i)
  }
}

# check correlation between predicted and actual values
out_correlation <- df_comparison %>%
  #filter(complete.cases(.)) %>%
  select(predicted, actual) %>%
  {cor(., method = 'spearman')[[2]]}
out_correlation

# show density plot
df_comparison %>%
  ggplot(aes(x = actual, y = predicted)) +
  geom_point(alpha = 0.2) +
  #geom_density2d_filled() +
  geom_smooth(method = 'gam', alpha = .5, color = 'coral') +
  facet_wrap(~ variable, scales = 'free') +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    # axis.text = element_blank(),
    # axis.ticks = element_blank(),
    axis.title = element_blank())
```

The results look promising for some metrics, like *functional group richness*
and *carbon stability* but less sensible for others such as
*probability of high severity fires*.


We need to add one more step to our approach: We want to model the likely values
of metrics after treatment. Direct impact will be simulated by moving the
metrics tree density and basal area, indirect impacts need to be estimated by
the model. However, we can use the values of the metric prior to treatment for
our estimates, because e.g. areas with low probability of severe fires will
still have a low severity afterwards, probably even lower!

That means we can use the pre-treatment values in our estimation, and we can
also include the knowledge on directionality (e.g. probability of high severity
fires will remain the same or decrease).


```{r}
# identify potential treatment rows via undesireable conditions
# in directly impacted metrics
archetype <- df_imputation %>%
  mutate(archetype = for_str_td > 0 & for_str_ba == 1) %>%
  pluck('archetype')

# static variables that don't change due to treatment
df_static <- df_imputation %>%
  select(climclass)
# pre-treatment conditions
df_pretreat <- df_imputation %>%
  select(-climclass) %>%
  set_colnames(paste0('pretreat_', colnames(.)))
# create table for post treatment values
df_posttreat <- df_imputation %>%
  select(-climclass) %>%
  set_colnames(paste0('posttreat_', colnames(.)))

direct_metrics <- c(
  'pretreat_for_str_td', 'pretreat_for_str_ba',
  'posttreat_for_str_td', 'posttreat_for_str_ba')
indirect_metrics <- c(
  colnames(df_pretreat),
  colnames(df_posttreat)) %>%
  setdiff(direct_metrics)

# set indirect metrics in  stands with desireable forest structure variables
# to NA in pre-treatment
df_pretreat[archetype, colnames(df_pretreat) %in% indirect_metrics] <- NA
# set indirect metrics in treatment stands to NA in post-treatment
df_posttreat[!archetype, colnames(df_posttreat) %in% indirect_metrics] <- NA
# set post treatment direct metrics in treatment stands to desired conditions
df_posttreat[!archetype, 'posttreat_for_str_td'] <- .1
df_posttreat[!archetype, 'posttreat_for_str_ba'] <- 1

# combine the tables
df_prediction <- bind_cols(df_static, df_pretreat, df_posttreat)

# start mice setup
ini <- mice(
  # use the full dataset instead this time
  data = df_prediction,
  # find metrics with minimum correlation for prediction
  pred = quickpred(df_prediction, mincor = .1),
  maxit = 0,
  remove.collinear = FALSE,
  print = FALSE)

# specify prediction relationship.
pred <- ini$predictorMatrix
# always use climate class
pred[, 'climclass'] <- 1

# specify post-processing logic for mice
adjust_iv <- function(imp, i, j) {
  squeeze(imp[[j]][, i], c(-1, 1))
}
post_processing <- ini$post
post_processing_logic <- 'imp[[j]][, i] <- adjust_iv(imp, i, j)'
post_processing[1:length(post_processing)-1] <- post_processing_logic

# run actual imputation
imputation <- mice(
  data = df_prediction,
  method = 'norm.predict',
  pred = pred,
  post = post_processing,
  m = 1,
  maxit = 10,
  seed = 42,
  remove.collinear = FALSE,
  print = FALSE)
df_completed <- imputation %>%
  complete() %>%
  tibble()

df_completed %>%
  # drop archetype rows
  filter(imputation$where[, 14:25] %>% rowSums == 10) %>%
  select(starts_with('posttreat')) %>%
  bind_rows(df_pretreat) %>%
  pivot_longer(everything()) %>%
  mutate(
    post = case_when(
      str_detect(name, '^post') ~ 'post treatment',
      TRUE ~ 'pre treatment'),
    name = name %>% str_remove('pretreat_') %>% str_remove('posttreat_')) %>%
  # TODO: remove
  filter(complete.cases(.)) %>%
  pivot_wider(
    names_from = post,
    values_from = value,
    values_fn = list(value = mean)) %>%
  select(name, `pre treatment`, `post treatment`) %>%
  mutate(delta = `post treatment` - `pre treatment`)
```

To be honest, the results don't seem to make a lot of sense. *carbon*
has increased when it surely should have gone down due to removal of biomass,
most other metrics have declined. I think the problem may be that we only
presented the archetype cells as "labelled data" to the model, thus inducing
bias into our forecast.

Let's try instead to use all stands as labeled data.

```{r}

df_rf <- tibble(
  metric = df_imputation |> select(-climclass) %>% colnames(),
  X = list(df_imputation)) |>
  mutate(
    y = map2(.x = X, .y = metric, .f = ~pluck(.x, .y)),
    X = map2(.x = X, .y = metric, .f = ~select(.x, -.y)))

library(randomForest)
library(glmnet)
library(broom)

df_rf <- df_rf %>%
  mutate(
    glmnet = map2(X, y, ~cv.glmnet(.x[[1]], .y[[1]])))
#    rf = map2(X, y, ~randomForest(.x, .y)))

df_rf <- df_rf %>%
  mutate(
    stats_glm = map(glmnet, tidy))

cv.glmnet(
  x = as.matrix(df_rf$X[[10]]),
  y = df_rf$y[[10]],
  type.measure = 'mse')
# 
# ?randomForest
# df_imputation
  
```





```{r}

# 
# # don't predict basal area, tree density, large tree density, or climate class
# # because these will be set manually
# pred[c(
#   'for_str_td',
#   'for_str_ba',
#   'for_str_ltd',
#   'climclass'), ] <- 0
# # don't use any of the indirectly impacted metrics
# # because they might interact with one-another
# pred[, c(
#  'for_com_se',
#  'for_com_sl',
#  'for_com_drid',
#  'fir_fun_tslf',
#  'fad_sev_hsp',
#  'bio_foc_cso',
#  'bio_div_sr',
#  'bio_com_fgr',
#  'co2_stb_stb')] <- 0
# 


```