---
title: "Prototype Analysis for Inferring Treatment Impact - ACCEL dataset"
author: "Laurens Geffert"
date: "2023-01-10"
output: rmarkdown::github_document
---

<!-- impact_of_treatment.md is generated from impact_of_treatment.Rmd Please edit that file -->

```{r, echo = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = '##',
  fig.retina = 2,
  fig.path = '../output/impact_of_treatment_files/impact_of_treatment_accel')
```

```{r setup, echo = FALSE, error = TRUE}
library(sf)
library(fs)
library(raster)
library(tidyverse)
library(tidymodels)
library(magrittr)
library(purrr)
library(zeallot)
library(mice)
library(corrr)
```


We're using the raw values from the ACCEL data set in this analysis and manually replicate the normalization step to avoid issues with bidirectional metrics. Some of the data needs some pre-processing. We're also including the raw climate variables that were used to calculate Climate Class because modelling will get better results when using continuous values rather than reclassified categorical variables. All layers are stacked and masked to the Sierra Nevada ACCEL area.

```{r defining_inputs, error = TRUE, run = FALSE}
# Using this raster as the target resolution and extent
default_raster <- raster('../data/ACCEL RRK - all layers/airQuality/particulate/PotentialSmokeHighSeverity_2021_300m_base.tif')
# TODO: Get standard extent raster from Nick

resample_if_needed <- function(r, default_raster, silent = TRUE, ...) {
  if (!all(dim(r) == dim(default_raster))) {
    if (!compareCRS(r, default_raster)) {
      message(paste('reprojecting raster', names(r)))
      # climate class layer has coded NA values
      if (names(r) == 'climClass') {
        r[r == 128] <- NA
      }
      r <- projectRaster(from = r, to = default_raster, method = 'ngb')
    }
    message(paste('resampling raster', names(r)))
    r <- raster::resample(x = r, y = default_raster, 'ngb')
  } else {
    if (!silent) message(paste('no need to resample', names(r)))
  }
  return(r)
}

# getting file paths of all metrics
metrics <- tibble(
  filename = '../data/ACCEL RRK - all layers/' |>
    dir_ls(recurse = TRUE) |>
    str_subset('(\\.tif$)|(\\.img$)'))

# getting name hierarchy
metrics <- metrics |>
  mutate(
    subfolder = str_extract(
      string = filename,
      pattern = '(?<=..\\/data\\/ACCEL RRK - all layers\\/).+(?=(\\.tif$)|(\\.img$))')) |>
  separate(
    col = subfolder,
    into = c('pillar', 'element', 'folder', 'metric'),
    sep = '/',
    fill = 'left') |>
  mutate(
    folder = coalesce(folder, metric),
    element = coalesce(element, folder),
    pillar = coalesce(pillar, element)) |>
  # filtering out 30m rasters
  filter(str_detect(metric, '30m', negate = TRUE)) |>
  # filtering out normalized rasters
  filter(str_detect(metric, '[Nn]ormalized', negate = TRUE))

# adding climate variables
metrics <- tibble(
  filename = c(
    "../data/Climate Class/input rasters/aet1981_2010.tif",
    "../data/Climate Class/input rasters/cwd1981_2010.tif",
    "../data/Climate Class/input rasters/tmn1981_2010.tif",
    "../data/Climate Class/input rasters/tmx1981_2010.tif"),
  pillar = 'climate',
  element = 'climate',
  folder = 'climate',
  metric = 'climate') |>
  bind_rows(metrics)

# adding ACCEL mask
metrics <- tibble(
  filename = '../data/ACCEL_MASK.tif',
  pillar = 'mask',
  element = 'mask',
  folder = 'mask',
  metric = 'mask') |>
  bind_rows(metrics)

# loading rasters
rasters <- metrics |>
  mutate(r = filename |>
           map(.f = raster) |>
           map2(.y = metric, .f = ~set_names(.x, .y))) |>
  mutate(r = map(r, resample_if_needed, default_raster = default_raster))

# persisting data for faster restarting from here
write_rds(
  x = rasters,
  file = '../output/rrk_rasters_raw_processed.rds')
```


```{r loading_data, error = TRUE}
source('../code/utils.R')

# restoring pre-loaded data
rasters <- read_rds(file = '../output/rrk_rasters_raw_processed.rds')

# converting raster values into dataframe
df <- rasters %>%
  pluck('r') %>%
  stack() %>%
  values() %>%
  as_tibble() %>%
  # dropping anything outside of study area
  filter(!is.na(mask))
  
# checking missingness and generate breakdown per variable
df_missingness <- df %>%
  mutate_all(is.na) %>%
  # counting NA cells per metric
  {tibble(
    metric = colnames(.),
    n_na = colSums(.),
    perc_na = colSums(.) / nrow(.) * 100)} %>%
  arrange(desc(perc_na))

df_normalized <- df %>%
  dplyr::select(
    # removing variables that are not needed anymore
    -mask,
    # removing variables that are problematic due to missing values
    -Meadow_SensNDWI_2019_300m_base,
    -DamagePotential_WUI_2022_300m_base,
    -DamagePotential_WUI_2022,
    -StructureExposureScore_WUI_2022_300m_base,
    -StructureExposureScore_WUI_2022,
    # removing socioeconomic variables that will be difficult to predict
    -LowIncome_CCI_2021_300m_base,
    -UnemploymentPctl_2021_300m_base,
    -HousingBurdenPctl_2021_300m_base,
    # removing redundant variables
    -ACCEL_habitatConnectivity_valuesInt,
    -CA_Black_Oak_Stand_Distribution_2016to2020,
    -CA_Black_Oak_Stand_Distribution_2016to2020,
    # and removing some more
    -ACCEL_habitatConnectivity_values_300m,
    -skidder_bio_cost_proj_clip_300m_base,
    -skidder_saw_cost_proj_clip2_300m_base,
    -xmechctrl_8_2022_300m,
    -Mortality_MMI_2017_2021_300m,
    -Mortality_MMI_2017_2021_compressed,
    -meanPFRID_300m) %>%
  # engineering new variables for small trees and non-tree biomass
  mutate(
    biomass_small =
      AvailableBiomass_2021_300m_base -
      LargeTreeCarbon_2021_300m_base,
    TPA_small =
      TPA_2021_300m_base -
      TPA_30in_up_2021_300m_base) %>%
  # removing outliers
  mutate(across(
    PotentialSmokeHighSeverity_2021_300m_base:TPA_small,
    remove_outliers)) %>%
  # normalizing values to -1 to 1 range
  mutate(across(
    PotentialSmokeHighSeverity_2021_300m_base:TPA_small,
    normalize_values))
```

```{r filtering_data, error = TRUE}
# drop all variables that are mostly NA
df_normalized <- df_normalized %>%
  # dropping all rows that are mostly NA
  filter(rowSums(!is.na(.)) > 4)

# creating test and validation split
set.seed(42)
data_split <- initial_split(df_normalized, prop = 3/4)
df_train <- training(data_split)
df_test  <- testing(data_split)
```


Here we plot a histogram of the values for each metric. We're using only the interpreted values (range -1 to 1), except for climate. You can see that some of the variables are approximately normally distributed while others look almost like binomial distributions.

```{r plot_metric_histogram, error = TRUE}
# plotting a histogram for each metric
df_train %>%
  pivot_longer(everything()) %>%
  filter(complete.cases(.)) %>%
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30) +
  facet_wrap(~name, scales = 'free')
```


```{r}
# showing correlations between variables
df_train %>%
  correlate() %>%
  rplot()
```


We can investigate correlations between metrics with a pair plot collection of scatterplots. We're also adding a trend line to make it easier to spot relationships between the metrics where many observations fall into the same value range.

```{r}
# investigate correlations between metrics with a pair plot collection
df_train %>%
  sample_n(1e4) %>%
  select(one_of(
    'AvailableBiomass_2021_300m_base',
    'BASATOT_2021_300m_base',
    'LargeTreeCarbon_2021_300m_base',
    'TPA_2021_300m_base',
    'TPA_30in_up_2021_300m_base',
    'TPA_small'
  )) %>%
  # self-join data to get pairs of observations
  mutate(row_id = row_number()) %>%
  pivot_longer(cols = -row_id) %>%
  full_join(., ., by = 'row_id') %>%
  # create pair plot
  ggplot( aes(x = value.x, y = value.y)) + 
  geom_point(alpha = .02) +
  geom_smooth(method = 'gam', alpha = .2, color = 'coral') +
  facet_grid(name.x ~ name.y, scales = 'free', switch = 'y') +
  theme_minimal() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    axis.title = element_blank())
```


Let's build a model to capture the relationship between the different metrics. We will use a selection of "independent" and "direct impact" metrics as predictors. Independent metrics are what we consider metrics which are unlikely to change due to management, namely climatic variables. Direct impact metrics that can be moved through management intervention, namely total basal area, trees per acre, and large trees per acre. Using these predictors we build a model for each indirectly impacted metric to estimate the strength of relationship.

```{r modeling}
# training models for each metric
source('../code/utils.R')

modifiable <- c(
  'AvailableBiomass_2021_300m_base',
  'BASATOT_2021_300m_base',
  'LargeTreeCarbon_2021_300m_base',
  'TPA_2021_300m_base',
  'TPA_30in_up_2021_300m_base',
  'TPA_small'
  )
# building models for each metric
predictors <- c(
  'climate.1',
  'climate.2',
  'climate.3',
  'climate.4',
  'LargeTreeCarbon_2021_300m_base',
  'TPA_30in_up_2021_300m_base',
  'TPA_small'
  )
x <- df_train %>%
  # generating a table with one row per response metric
  #select(-one_of(predictors)) %>%
  colnames() %>%
  tibble(metric = .) %>%
  # removing metrics that aren't meaningful to predict
  filter(!metric %in% modifiable) %>%
  filter(!metric %in% predictors) %>%
  filter(!str_detect(metric, '^climate')) %>%
  # getting training response variable
  mutate(train_y = map(metric, ~pluck(df_train, .x))) %>%
  mutate(train_useable = map2(
    .x = train_y,
    .y = metric,
    .f = find_useable_observations,
    df = df_train,
    predictors = predictors)) %>%
  # getting test response variable
  mutate(test_y = map(metric, ~pluck(df_test, .x))) %>%
  mutate(test_useable = pmap(.,
    find_useable_observations,
    df = df_test,
    response = test_y,
    predictors = predictors)) %>%
  # training a model for each metric
  mutate(model = map2(
    .x = train_useable,
    .y = train_y,
    .f = fit_glmnet,
    df = df_train,
    predictors = predictors)) %>%
  mutate(coeff = pmap(., get_coefficients)) %>%
  # making prediction on test data
  mutate(pred = pmap(.,
    predict_values,
    df = df_test,
    predictors = predictors)) %>%
  mutate(cod = pmap_dbl(., calculate_cod))
```


Quality of model outputs can be checked using the coefficient of determination for each model. Here we're returning that in tabular format.

```{r modeling_check}
# looking and strength of prediction
cod <- x %>%
  select(metric, cod) %>%
  arrange(desc(cod)); print(cod, n = 40)
```


Finally, we write out the results of our modeling effort as a csv containing the coefficients for each predictor, but leaving out the climatic variables since these were just included to account for site-specific differences.

```{r modeling_results}
# extracting coefficients for direct impact variables
coeffs <- x %>%
  select(metric, cod, coeff) %>%
  unnest_wider(coeff) %>%
  select(-starts_with('climate'), -`(Intercept)`); print(coeffs, n = 40)

write_csv(coeffs, '../output/impact_of_treatment_models.csv')
```











